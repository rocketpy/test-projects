import csv
import requests
from bs4 import BeautifulSoup
from requests.exceptions import HTTPError
from requests_jwt import JWTAuth


useragent = 'Mozilla/5.0 (Windows; U; MSIE 9.0; WIndows NT 9.0; en-US)'
headers = {'User-Agent': useragent}
# headers = {'accept':'*/*', 'user-agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64; rv:50.0) Gecko/20100101 Firefox/50.0'}
# head = {'Authorization': 'Bearer {}'.format(myToken)}

auth = JWTAuth('MySecretToken')
requests.get("http://", auth=auth)
s = requests.Session()
req = s.get(url, headers=headers)

r = s.get("https://")


"""
import python_jwt as jwt
# Create claims dictionary for generation of JwToken
claims = {'consumerId': 'My App ID',
          'httpMethod': 'GET'}

import datetime
# create JWToken
jwtoken = jwt.generate_jwt(claims, 'My secret', 'HS256', datetime.timedelta(minutes=5))

response = requests.get('http://httpbin.org/get', jwtoken)
print(response.json())
"""

"""
proxies = {
  'http': 'http://10.10.1.10:3128',
  'https': 'http://10.10.1.10:1080',
}
requests.get('http://', proxies=proxies)
"""
# using proxy
# session = requests.Session()
# session.proxies.update(proxies)
# session.get('http://')


def save_cookies(requests_cookiejar, filename):
    with open(filename, 'wb') as f:
        pickle.dump(requests_cookiejar, f)


def load_cookies(filename):
    with open(filename, 'rb') as f:
        return pickle.load(f)
    
    
"""
s = requests.Session()
headers = {'User-Agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_5)'\
           'AppleWebKit 537.36 (KHTML, like Gecko) Chrome',
           'Accept':'text/html,application/xhtml+xml,application/xml;'\
           'q=0.9,image/webp,*/*;q=0.8'}
url = 'https:'
req = session.get(url, headers=headers)

s.get('https://')
r = s.get("https://")
print(r.text)
"""

# save cookies
r = requests.get(url)
save_cookies(r.cookies, filename='cookies')        

# load cookies and do a request
r = requests.get(url, cookies=load_cookies(filename='cookies'))
# print(r.json())


def write_csv(data):
    with open('result.csv', 'a') as f:
        order = ['Doctor', 'Work time', 'Address', 'Rating']
        writer = csv.DictWriter(f, fieldnames=order)
        writer.writerow(data)


def get_data():
        with open('data.txt') as json_file:
        data = json.load(json_file)
        # print(data)
        print(json.dumps(data, indent=4, sort_keys=True))


def main():
    # url = 'https://www.zooplus.de/tierarzt/results?animal_99=true'
    for i in range(1, 4):
        try:
            url = f'https://www.zooplus.de/tierarzt/results?animal_99=true%27&page={i}'
            response = requests.get(url)
            response.raise_for_status()
        except HTTPError as http_err:
            print(f'HTTP error occurred: {http_err}')
        except Exception as err:
            print(f'Other error occurred: {err}')
        else:
            print('Success!')
        # print(get_data(get_html(url)))


if __name__ == '__main__':
    main()
    
    

"""
# for multiprocessing
def make_all(url):
    text = get_html(url)
    get_page_data(text)

def main():
    url = ''
    urls = [url.format(str(i)) for i in range(1, 4)]

    with Pool(20) as p:  # pools is a process
        p.map(make_all, urls)  # p.map to every func make_all
"""
